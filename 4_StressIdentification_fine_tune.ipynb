{"cells":[{"cell_type":"markdown","metadata":{"id":"OYpJoHsns0aA"},"source":["## Opis problemu\n","\n","Znajd藕 dowolny zbi贸r danych (dozwolone jzyki: angielski, hiszpaski, polski, szwedzki) (poza IMDB oraz zbiorami wykorzystywanymi na zajciach) do analizy sentymentu.\n","Zbi贸r mo偶e mie 2 lub 3 klasy.\n","\n","Nastpnie:\n","1. Oczy dane i zaprezentuj rozkad klas\n","2. Zbuduj model analizy sentymenu:\n","  - z wykorzystaniem sieci rekurencyjnej (LSTM/GRU/sie dwukierunkowa) innej ni偶 podstawowe RNN\n","  - z wykorzystaniem sieci CNN\n","  - z podstawiemiem pre-trained word embedding贸w\n","  - z fine-tuningiem modelu jzyka (poza podstawowym BERTem)\n","\n","3. Stw贸rz funkcj, kt贸ra bdzie korzystaa z wytrenowanego modelu i zwracaa wynik dla przekazanego pojedynczego zdania (zda) w postaci komunikatu informujcego u偶ytkownika, czy tekst jest nacechowany negatywnie, pozytywnie (czy neutralnie w przypadku 3 klas).\n","\n","4. Gotowe rozwizanie zamie na GitHubie z README. W README zawrzyj: informacje o danych - ich pochodzenie, oraz opis wybranego modelu i instrukcje korzystania z plik贸w.\n","5. W assigmnencie w Teamsach wrzu link do repo z rozwizaniem. W przypadku prywatnego repo upewnij si, 偶e bdzie ono widoczne dla `dwnuk@pjwstk.edu.pl`.\n","\n","**TERMIN**: jak w Teamsach"]},{"cell_type":"markdown","metadata":{},"source":["1. Oczy dane i zaprezentuj rozkad klas"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!pip install -U accelerate -q\n","!pip install -U transformers -q"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["!pip install -U datasets -q"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Administrator\\Desktop\\-NLP-Stress-Identification\\StressEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["'4.36.2'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import transformers\n","transformers.__version__"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>subreddit</th>\n","      <th>post_id</th>\n","      <th>sentence_range</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>confidence</th>\n","      <th>social_timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ptsd</td>\n","      <td>8601tu</td>\n","      <td>(15, 20)</td>\n","      <td>He said he had not felt that way before, sugge...</td>\n","      <td>1</td>\n","      <td>0.800000</td>\n","      <td>1521614353</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>assistance</td>\n","      <td>8lbrx9</td>\n","      <td>(0, 5)</td>\n","      <td>Hey there r/assistance, Not sure if this is th...</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","      <td>1527009817</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ptsd</td>\n","      <td>9ch1zh</td>\n","      <td>(15, 20)</td>\n","      <td>My mom then hit me with the newspaper and it s...</td>\n","      <td>1</td>\n","      <td>0.800000</td>\n","      <td>1535935605</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>relationships</td>\n","      <td>7rorpp</td>\n","      <td>[5, 10]</td>\n","      <td>until i met my new boyfriend, he is amazing, h...</td>\n","      <td>1</td>\n","      <td>0.600000</td>\n","      <td>1516429555</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>survivorsofabuse</td>\n","      <td>9p2gbc</td>\n","      <td>[0, 5]</td>\n","      <td>October is Domestic Violence Awareness Month a...</td>\n","      <td>1</td>\n","      <td>0.800000</td>\n","      <td>1539809005</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2833</th>\n","      <td>relationships</td>\n","      <td>7oee1t</td>\n","      <td>[35, 40]</td>\n","      <td>* Her, a week ago: Precious, how are you? (I i...</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","      <td>1515187044</td>\n","    </tr>\n","    <tr>\n","      <th>2834</th>\n","      <td>ptsd</td>\n","      <td>9p4ung</td>\n","      <td>[20, 25]</td>\n","      <td>I don't have the ability to cope with it anymo...</td>\n","      <td>1</td>\n","      <td>1.000000</td>\n","      <td>1539827412</td>\n","    </tr>\n","    <tr>\n","      <th>2835</th>\n","      <td>anxiety</td>\n","      <td>9nam6l</td>\n","      <td>(5, 10)</td>\n","      <td>In case this is the first time you're reading ...</td>\n","      <td>0</td>\n","      <td>1.000000</td>\n","      <td>1539269312</td>\n","    </tr>\n","    <tr>\n","      <th>2836</th>\n","      <td>almosthomeless</td>\n","      <td>5y53ya</td>\n","      <td>[5, 10]</td>\n","      <td>Do you find this normal? They have a good rela...</td>\n","      <td>0</td>\n","      <td>0.571429</td>\n","      <td>1488938143</td>\n","    </tr>\n","    <tr>\n","      <th>2837</th>\n","      <td>ptsd</td>\n","      <td>5y25cl</td>\n","      <td>[0, 5]</td>\n","      <td>I was talking to my mom this morning and she s...</td>\n","      <td>1</td>\n","      <td>0.571429</td>\n","      <td>1488909516</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2838 rows  7 columns</p>\n","</div>"],"text/plain":["             subreddit post_id sentence_range  \\\n","0                 ptsd  8601tu       (15, 20)   \n","1           assistance  8lbrx9         (0, 5)   \n","2                 ptsd  9ch1zh       (15, 20)   \n","3        relationships  7rorpp        [5, 10]   \n","4     survivorsofabuse  9p2gbc         [0, 5]   \n","...                ...     ...            ...   \n","2833     relationships  7oee1t       [35, 40]   \n","2834              ptsd  9p4ung       [20, 25]   \n","2835           anxiety  9nam6l        (5, 10)   \n","2836    almosthomeless  5y53ya        [5, 10]   \n","2837              ptsd  5y25cl         [0, 5]   \n","\n","                                                   text  label  confidence  \\\n","0     He said he had not felt that way before, sugge...      1    0.800000   \n","1     Hey there r/assistance, Not sure if this is th...      0    1.000000   \n","2     My mom then hit me with the newspaper and it s...      1    0.800000   \n","3     until i met my new boyfriend, he is amazing, h...      1    0.600000   \n","4     October is Domestic Violence Awareness Month a...      1    0.800000   \n","...                                                 ...    ...         ...   \n","2833  * Her, a week ago: Precious, how are you? (I i...      0    1.000000   \n","2834  I don't have the ability to cope with it anymo...      1    1.000000   \n","2835  In case this is the first time you're reading ...      0    1.000000   \n","2836  Do you find this normal? They have a good rela...      0    0.571429   \n","2837  I was talking to my mom this morning and she s...      1    0.571429   \n","\n","      social_timestamp  \n","0           1521614353  \n","1           1527009817  \n","2           1535935605  \n","3           1516429555  \n","4           1539809005  \n","...                ...  \n","2833        1515187044  \n","2834        1539827412  \n","2835        1539269312  \n","2836        1488938143  \n","2837        1488909516  \n","\n","[2838 rows x 7 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","data = pd.read_csv('Stress.csv')\n","data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1206</th>\n","      <td>Public speaking in class frequently reduced me...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>163</th>\n","      <td>I hate the thought that even after my mom's de...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1132</th>\n","      <td>You can read the full terms and instructions h...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2144</th>\n","      <td>Clearly he's hurting inside and I want to get ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>457</th>\n","      <td>Im noticing a pattern where my body is like r...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   text  label\n","1206  Public speaking in class frequently reduced me...      1\n","163   I hate the thought that even after my mom's de...      1\n","1132  You can read the full terms and instructions h...      0\n","2144  Clearly he's hurting inside and I want to get ...      1\n","457   Im noticing a pattern where my body is like r...      1"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["cols_to_drop = ['subreddit','post_id','sentence_range','confidence','social_timestamp']\n","df = data.drop(cols_to_drop,axis=1)\n","df.sample(5)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from datasets import Dataset\n","\n","dataset_ = Dataset.from_pandas(df)\n","dataset = dataset_.train_test_split(0.1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 2554\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 284\n","    })\n","})\n"]}],"source":["print(dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from transformers import RobertaModel, RobertaTokenizer\n","\n","model_checkpoint = 'distilbert-base-uncased'\n","batch_size = 32"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|| 2554/2554 [00:01<00:00, 1555.12 examples/s]\n","Map: 100%|| 284/284 [00:00<00:00, 1441.78 examples/s]\n"]}],"source":["def process(x):\n","  return tokenizer(x['text'])\n","\n","train_ds = dataset['train'].map(process)\n","test_ds = dataset['test'].map(process)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'attention_mask'],\n","    num_rows: 2554\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_ds"]},{"cell_type":"markdown","metadata":{},"source":["2. Zbuduj model analizy sentymenu:\n","  - z wykorzystaniem sieci rekurencyjnej (LSTM/GRU/sie dwukierunkowa) innej ni偶 podstawowe RNN\n","  - z wykorzystaniem sieci CNN\n","  - z podstawiemiem pre-trained word embedding贸w\n","  - z fine-tuningiem modelu jzyka (poza podstawowym BERTem) <---"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Administrator\\Desktop\\-NLP-Stress-Identification\\StressEnv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","num_labels = 2\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["args = TrainingArguments(\n","    f'{model_checkpoint}_sentiment_analysis',\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    learning_rate = 2e-5,\n","    per_device_train_batch_size = batch_size,\n","    per_device_eval_batch_size = batch_size,\n","    num_train_epochs = 5,\n","    weight_decay = 0.1,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'accuracy'\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6744\\910376946.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric('glue', 'sst2')\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Administrator\\Desktop\\-NLP-Stress-Identification\\StressEnv\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]}],"source":["from datasets import load_metric\n","import numpy as np\n","\n","metric = load_metric('glue', 'sst2')\n","\n","def compute_metric(eval_preds):\n","  logits, labels = eval_preds\n","  predictions = np.argmax(logits, axis=-1)\n","  return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_ds,\n","    eval_dataset=test_ds,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metric\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","100%|| 1/1 [00:00<00:00, 40.55it/s]\n"]},{"data":{"text/plain":["{'eval_loss': 0.6809222102165222,\n"," 'eval_accuracy': 1.0,\n"," 'eval_runtime': 0.2004,\n"," 'eval_samples_per_second': 4.99,\n"," 'eval_steps_per_second': 4.99}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate([train_ds[0]])"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                  \n"," 20%|        | 80/400 [31:24<1:40:37, 18.87s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.38694098591804504, 'eval_accuracy': 0.8450704225352113, 'eval_runtime': 76.4719, 'eval_samples_per_second': 3.714, 'eval_steps_per_second': 0.118, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                                     \n"," 40%|      | 160/400 [1:02:27<1:11:21, 17.84s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.4153191149234772, 'eval_accuracy': 0.8169014084507042, 'eval_runtime': 76.9055, 'eval_samples_per_second': 3.693, 'eval_steps_per_second': 0.117, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                                     \n"," 60%|    | 240/400 [1:33:59<51:28, 19.31s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.36661282181739807, 'eval_accuracy': 0.8415492957746479, 'eval_runtime': 77.0714, 'eval_samples_per_second': 3.685, 'eval_steps_per_second': 0.117, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                                     \n"," 80%|  | 320/400 [2:05:34<26:44, 20.05s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.3899497985839844, 'eval_accuracy': 0.8274647887323944, 'eval_runtime': 76.7012, 'eval_samples_per_second': 3.703, 'eval_steps_per_second': 0.117, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["                                                     \n","100%|| 400/400 [2:36:22<00:00, 21.20s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.43250441551208496, 'eval_accuracy': 0.8345070422535211, 'eval_runtime': 76.7024, 'eval_samples_per_second': 3.703, 'eval_steps_per_second': 0.117, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|| 400/400 [2:36:24<00:00, 23.46s/it]"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 9384.1496, 'train_samples_per_second': 1.361, 'train_steps_per_second': 0.043, 'train_loss': 0.29859272003173826, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["TrainOutput(global_step=400, training_loss=0.29859272003173826, metrics={'train_runtime': 9384.1496, 'train_samples_per_second': 1.361, 'train_steps_per_second': 0.043, 'train_loss': 0.29859272003173826, 'epoch': 5.0})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer.model"]},{"cell_type":"markdown","metadata":{},"source":["3. Stw贸rz funkcj, kt贸ra bdzie korzystaa z wytrenowanego modelu i zwracaa wynik dla przekazanego pojedynczego zdania (zda) w postaci komunikatu informujcego u偶ytkownika, czy tekst jest nacechowany negatywnie, pozytywnie (czy neutralnie w przypadku 3 klas)."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","def predict_sentiment(text, model):\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","    tokens = tokenizer(text, return_tensors='pt')\n","    input_ids = tokens['input_ids']\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    input_ids = input_ids.to(device)\n","\n","    model = model.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","\n","    logits = outputs.logits\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","\n","    if predicted_class == 1:\n","        return \"I can sense STRESS in this sentence\"\n","    else:\n","        return \"All good don't sense ANY STRESS in here \""]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["All good don't sense ANY STRESS in here \n"]}],"source":["text_example = \"I had a peaceful evening reading my favorite book.\"\n","result = predict_sentiment(text_example, trainer.model)\n","print(result)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNUgSmvEYYrPFXtRovGNFr2","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
